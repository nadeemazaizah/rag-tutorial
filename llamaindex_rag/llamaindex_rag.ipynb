{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "from llama_index.vector_stores.lancedb import LanceDBVectorStore\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.core.response.notebook_utils import (\n",
    "    display_source_node,\n",
    "    display_response,\n",
    ")\n",
    "from llama_index.core.extractors import (\n",
    "    SummaryExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    "    TitleExtractor,\n",
    "    KeywordExtractor,\n",
    ")\n",
    "from llama_index.extractors.entity import EntityExtractor\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "import Stemmer\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "# apply nested async to run in a notebook\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure OpenAI LLM and Embedding connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureOpenAI(\n",
    "    engine=os.getenv(\"AZURE_OPENAI_LLM_DEPLOYMENT\"),\n",
    "    model=os.getenv(\"AZURE_OPENAI_LLM_DEPLOYMENT\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_LLM_API_VERSION\"),\n",
    ")\n",
    "\n",
    "\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"),\n",
    "    deployment_name=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_EMBEDDING_API_VERSION\"),\n",
    ")\n",
    "\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama for open source LLM and Embedding models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama for local and open source models: https://ollama.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama_llm = Ollama(model=\"llama3.1:latest \", request_timeout=600.0)\n",
    "\n",
    "# ollama_embed_model = OllamaEmbedding(\n",
    "#     model_name=\"nomic-embed-text:latest\",\n",
    "#     base_url=\"http://localhost:11434\"\n",
    "# )\n",
    "\n",
    "# Settings.llm = ollama_llm\n",
    "# Settings.embed_model = ollama_embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pdf file/s from data folder:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the following pdf:\n",
    "https://arxiv.org/pdf/2501.12948"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles_folder = \"../data\"\n",
    "documents = SimpleDirectoryReader(profiles_folder).load_data()\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split text by sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize node parser\n",
    "splitter = SentenceSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "print(len(nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split with more metadata:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.llamaindex.ai/en/stable/module_guides/indexing/metadata_extraction/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = [\n",
    "    SentenceSplitter(chunk_size=500, chunk_overlap=50),\n",
    "    TitleExtractor(nodes=3),\n",
    "    QuestionsAnsweredExtractor(questions=3),\n",
    "    SummaryExtractor(summaries=[\"prev\", \"self\", \"next\"]),\n",
    "    KeywordExtractor(keywords=10),\n",
    "    # EntityExtractor(prediction_threshold=0.5),\n",
    "]\n",
    "pipeline = IngestionPipeline(transformations=transformations)\n",
    "\n",
    "nodes = pipeline.run(documents=documents)\n",
    "print(len(nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question which we will use for the retrieve and query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"which reinforcement learning algorithm used in the deepseek r1 training?\",\n",
    "    \"can you explain how deepseek r1 model was trained?\",\n",
    "    \"can you explain more about the cold start?\",\n",
    "    \"list all the unsuccessful attempts mentioned in the paper?\",\n",
    "    \"can you list all the advantages of the deepseek r1 model over over models?\",\n",
    "    \"can explain in detail about reward modeling in which part of the paper it was mentioned?\",\n",
    "    \"list all the steps which performed to train the deepseek r1 model?\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "question = questions[3]\n",
    "question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key words search using BM25:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.llamaindex.ai/en/stable/examples/retrievers/bm25_retriever/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    nodes=nodes,\n",
    "    similarity_top_k=10,\n",
    "    stemmer=Stemmer.Stemmer(\"english\"),\n",
    "    language=\"english\",\n",
    ")\n",
    "bm25_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retrieved_nodes = bm25_retriever.retrieve(question)\n",
    "for node in bm25_retrieved_nodes:\n",
    "    display_source_node(node, source_length=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_query_engine = RetrieverQueryEngine.from_args(bm25_retriever)\n",
    "\n",
    "response = bm25_query_engine.query(question)\n",
    "display_response(\n",
    "    response, source_length=5000, show_source=True, show_source_metadata=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector Search:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.llamaindex.ai/en/v0.10.33/examples/vector_stores/SimpleIndexDemo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = LanceDBVectorStore(\n",
    "    uri=\"./lancedb\", mode=\"overwrite\", query_type=\"vector\"\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "vector_index = VectorStoreIndex(\n",
    "    nodes=nodes, show_progress=True, storage_context=storage_context\n",
    ")\n",
    "# advanced_nodes\n",
    "# configure retriever\n",
    "vector_retriever = VectorIndexRetriever(\n",
    "    index=vector_index,\n",
    "    similarity_top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_retrieved_nodes = vector_retriever.retrieve(question)\n",
    "for node in vector_retrieved_nodes:\n",
    "    display_source_node(node, source_length=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_query_engine = RetrieverQueryEngine.from_args(vector_retriever)\n",
    "\n",
    "response = vector_query_engine.query(question)\n",
    "display_response(\n",
    "    response, source_length=5000, show_source=True, show_source_metadata=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hybrid Search:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.llamaindex.ai/en/stable/examples/retrievers/reciprocal_rerank_fusion/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_retriever = QueryFusionRetriever(\n",
    "    [vector_retriever, bm25_retriever],\n",
    "    similarity_top_k=10,\n",
    "    num_queries=1,  # set this to 1 to disable query generation\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    use_async=False,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_retrieved_nodes = hybrid_retriever.retrieve(question)\n",
    "for node in hybrid_retrieved_nodes:\n",
    "    display_source_node(node, source_length=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_query_engine = RetrieverQueryEngine.from_args(hybrid_retriever)\n",
    "\n",
    "response = hybrid_query_engine.query(question)\n",
    "\n",
    "display_response(\n",
    "    response, source_length=5000, show_source=True, show_source_metadata=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View Prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prompt viewing function\n",
    "def display_prompt_dict(prompts_dict):\n",
    "    for k, p in prompts_dict.items():\n",
    "        text_md = f\"**Prompt Key**: {k}<br>\" f\"**Text:** <br>\"\n",
    "        display(Markdown(text_md))\n",
    "        print(p.get_template())\n",
    "        display(Markdown(\"<br><br>\"))\n",
    "\n",
    "\n",
    "prompts_dict = hybrid_query_engine.get_prompts()\n",
    "display_prompt_dict(prompts_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced Links:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.llamaindex.ai/en/stable/examples/retrievers/auto_merging_retriever/\n",
    "\n",
    "https://docs.llamaindex.ai/en/stable/examples/workflow/rag/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
